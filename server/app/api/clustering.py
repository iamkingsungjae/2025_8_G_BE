"""í´ëŸ¬ìŠ¤í„°ë§ API ì—”ë“œí¬ì¸íŠ¸"""
import sys
import traceback

try:
    from fastapi import APIRouter, HTTPException, Depends
except Exception as e:
    traceback.print_exc(file=sys.stderr)
    raise

try:
    from pydantic import BaseModel
except Exception as e:
    traceback.print_exc(file=sys.stderr)
    raise

from typing import List, Optional, Dict, Any
from sqlalchemy.ext.asyncio import AsyncSession
import pandas as pd
import numpy as np
import uuid
import json
import logging
from pathlib import Path
from collections import Counter
import os
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import NearestNeighbors
from sklearn.metrics import pairwise_distances

try:
    from app.db.session import get_session
except Exception as e:
    traceback.print_exc(file=sys.stderr)
    raise

try:
    from app.db.dao_panels import extract_features_for_clustering
except Exception as e:
    traceback.print_exc(file=sys.stderr)
    raise

try:
    from app.clustering.integrated_pipeline import IntegratedClusteringPipeline
except Exception as e:
    traceback.print_exc(file=sys.stderr)
    raise

try:
    from app.clustering.data_preprocessor import preprocess_for_clustering
except Exception as e:
    traceback.print_exc(file=sys.stderr)
    raise

try:
    from app.clustering.filters.panel_filter import PanelFilter
except Exception as e:
    traceback.print_exc(file=sys.stderr)
    raise

try:
    from app.clustering.processors.vector_processor import VectorProcessor
except Exception as e:
    traceback.print_exc(file=sys.stderr)
    raise

try:
    from app.clustering.algorithms.hdbscan import HDBSCANAlgorithm
except Exception as e:
    traceback.print_exc(file=sys.stderr)
    raise

try:
    from app.clustering.artifacts import save_artifacts, new_session_dir
except Exception as e:
    traceback.print_exc(file=sys.stderr)
    raise

try:
    router = APIRouter(prefix="/api/clustering", tags=["clustering"])
except Exception as e:
    traceback.print_exc(file=sys.stderr)
    raise


class ClusterRequest(BaseModel):
    """í´ëŸ¬ìŠ¤í„°ë§ ìš”ì²­"""
    panel_ids: List[str]
    algo: str = "hdbscan"  # "hdbscan" only (KMeans and MiniBatchKMeans removed)
    n_clusters: Optional[int] = None  # Noneì´ë©´ ìë™ ì„ íƒ
    use_dynamic_strategy: bool = True  # ë™ì  ì „ëµ ì‚¬ìš© ì—¬ë¶€
    filter_params: Optional[Dict[str, Any]] = None
    processor_params: Optional[Dict[str, Any]] = None
    algorithm_params: Optional[Dict[str, Any]] = None
    sample_size: Optional[int] = None  # ìƒ˜í”Œë§ í¬ê¸° (Noneì´ë©´ ì „ì²´ ë°ì´í„° ì‚¬ìš©)


class CompareRequest(BaseModel):
    """ê·¸ë£¹ ë¹„êµ ìš”ì²­"""
    session_id: str
    c1: int
    c2: int


class UMAPRequest(BaseModel):
    """UMAP 2D ì¢Œí‘œ ìš”ì²­"""
    session_id: str
    sample: Optional[int] = None
    metric: str = "cosine"
    n_neighbors: int = 15
    min_dist: float = 0.1
    seed: Optional[int] = 0


@router.post("/cluster-from-csv")
async def cluster_from_csv(
    req: ClusterRequest
):
    """
    [DEPRECATED] CSV íŒŒì¼ì—ì„œ ì§ì ‘ í´ëŸ¬ìŠ¤í„°ë§ ì‹¤í–‰ (DB ì—°ë™ ì—†ì´)
    
    âš ï¸ ì´ ì—”ë“œí¬ì¸íŠ¸ëŠ” ë” ì´ìƒ ì‚¬ìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    ëª¨ë“  í´ëŸ¬ìŠ¤í„°ë§ì€ NeonDBë¥¼ í†µí•´ ìˆ˜í–‰ë©ë‹ˆë‹¤.
    ëŒ€ì‹  `/api/clustering/cluster` ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
    """
    logger = logging.getLogger(__name__)
    
    # Deprecated ì—”ë“œí¬ì¸íŠ¸ ê²½ê³ 
    logger.warning("[DEPRECATED] /cluster-from-csv ì—”ë“œí¬ì¸íŠ¸ëŠ” ë” ì´ìƒ ì‚¬ìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. /api/clustering/clusterë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")
    
    raise HTTPException(
        status_code=410,  # Gone
        detail={
            "error": "ì´ ì—”ë“œí¬ì¸íŠ¸ëŠ” ë” ì´ìƒ ì‚¬ìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.",
            "message": "ëª¨ë“  í´ëŸ¬ìŠ¤í„°ë§ì€ NeonDBë¥¼ í†µí•´ ìˆ˜í–‰ë©ë‹ˆë‹¤.",
            "alternative": "/api/clustering/cluster ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”."
        }
    )
    


async def _execute_clustering(
    df: pd.DataFrame,
    req: ClusterRequest,
    debug_info: Dict[str, Any],
    logger: logging.Logger
):
    """ê³µí†µ í´ëŸ¬ìŠ¤í„°ë§ ì‹¤í–‰ ë¡œì§"""
    # 3. ì•Œê³ ë¦¬ì¦˜ ì„ íƒ (HDBSCANë§Œ ì‚¬ìš©)
    algorithm = None
    if req.algo == "hdbscan" or req.algo == "auto":
        algorithm = HDBSCANAlgorithm()
    else:
        # HDBSCANë§Œ ì§€ì›
        algorithm = HDBSCANAlgorithm()
    
    # 4. íŒŒì´í”„ë¼ì¸ êµ¬ì„±
    pipeline = IntegratedClusteringPipeline(
        filter=PanelFilter(),
        processor=VectorProcessor(),
        algorithm=algorithm,
        use_dynamic_strategy=req.use_dynamic_strategy
    )
    
    debug_info['step'] = 'clustering'
    
    # 5. í´ëŸ¬ìŠ¤í„°ë§ ì‹¤í–‰
    try:
        # mb_sn ì»¬ëŸ¼ ì œì™¸í•˜ê³  í´ëŸ¬ìŠ¤í„°ë§
        df_for_clustering = df.drop(columns=['mb_sn']) if 'mb_sn' in df.columns else df
        result = pipeline.cluster(df_for_clustering, verbose=False)
        logger.info(f"[í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ] ì„±ê³µ: {result.get('success', False)}")
    except Exception as clustering_error:
        debug_info['errors'].append(f'í´ëŸ¬ìŠ¤í„°ë§ ì‹¤í–‰ ì‹¤íŒ¨: {str(clustering_error)}')
        logger.error(f"[í´ëŸ¬ìŠ¤í„°ë§ ì˜¤ë¥˜] {str(clustering_error)}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=json.dumps({
                "error": f"í´ëŸ¬ìŠ¤í„°ë§ ì‹¤í–‰ ì‹¤íŒ¨: {str(clustering_error)}",
                "debug": debug_info
            }, ensure_ascii=False)
        )
    
    debug_info['step'] = 'complete'
    
    if not result.get('success'):
        # í”„ë¡œíŒŒì¼ë§ ëª¨ë“œ
        return {
            "success": False,
            "session_id": None,
            "n_samples": result.get('n_samples', 0),
            "n_clusters": 0,
            "labels": [],
            "cluster_sizes": {},
            "meta": {
                "filter_info": result.get('filter_info'),
                "processor_info": result.get('processor_info'),
                "algorithm_info": result.get('algorithm_info'),
            },
            "profile": result.get('profile'),
            "reason": result.get('reason', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜'),
            "debug": {
                **debug_info,
                "sample_size": result.get('n_samples', 0),
                "warnings": debug_info.get('warnings', []) + [result.get('reason', '')]
            }
        }
    
    # 6. ì„¸ì…˜ ìƒì„± ë° ì•„í‹°íŒ©íŠ¸ ì €ì¥
    session_dir = new_session_dir()
    session_id = session_dir.name  # ë””ë ‰í† ë¦¬ ì´ë¦„ì´ session_id
    
    # labels ì¶”ì¶œ (ì—¬ëŸ¬ ë°©ë²• ì‹œë„)
    labels = None
    if result.get('labels') is not None:
        # resultì— labelsê°€ ì§ì ‘ ìˆëŠ” ê²½ìš°
        labels = result['labels']
        if hasattr(labels, 'tolist'):
            labels = labels.tolist()
        elif isinstance(labels, np.ndarray):
            labels = labels.tolist()
        elif not isinstance(labels, list):
            labels = list(labels)
    elif 'data' in result and isinstance(result['data'], pd.DataFrame):
        # DataFrameì— cluster ì»¬ëŸ¼ì´ ìˆëŠ” ê²½ìš°
        if 'cluster' in result['data'].columns:
            labels = result['data']['cluster'].tolist()
    
    if labels is None:
        labels = []
        logger.warning("[ì„¸ì…˜ ì €ì¥] labelsë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    # cluster_sizes ê³„ì‚°
    cluster_sizes = result.get('cluster_sizes', {})
    if not cluster_sizes and labels:
        # labelsì—ì„œ cluster_sizes ê³„ì‚°
        from collections import Counter
        label_counts = Counter(labels)
        cluster_sizes = {k: int(v) for k, v in label_counts.items() if k != -1}  # ë…¸ì´ì¦ˆ ì œì™¸, í‚¤ë¥¼ ì •ìˆ˜ë¡œ ìœ ì§€
    
    # n_clusters ê³„ì‚° (cluster_sizes ë˜ëŠ” labelsì—ì„œ)
    n_clusters = result.get('n_clusters', 0)
    if n_clusters == 0:
        if cluster_sizes:
            # cluster_sizesì—ì„œ ë…¸ì´ì¦ˆ(-1) ì œì™¸í•œ í´ëŸ¬ìŠ¤í„° ìˆ˜ ê³„ì‚°
            valid_clusters = [k for k in cluster_sizes.keys() if k != -1 and k != '-1' and k != '-1.0']
            n_clusters = len(valid_clusters)
            logger.info(f"[í´ëŸ¬ìŠ¤í„° ìˆ˜ ê³„ì‚°] cluster_sizesì—ì„œ ê³„ì‚°: {n_clusters}ê°œ (keys: {list(cluster_sizes.keys())[:10]})")
        elif labels:
            # labelsì—ì„œ ê³ ìœ  í´ëŸ¬ìŠ¤í„° ìˆ˜ ê³„ì‚°
            unique_labels = set(labels)
            unique_labels.discard(-1)  # ë…¸ì´ì¦ˆ ì œì™¸
            n_clusters = len(unique_labels)
            logger.info(f"[í´ëŸ¬ìŠ¤í„° ìˆ˜ ê³„ì‚°] labelsì—ì„œ ê³„ì‚°: {n_clusters}ê°œ (unique: {sorted(unique_labels)[:10]})")
    
    logger.info(f"[í´ëŸ¬ìŠ¤í„° ìˆ˜ ìµœì¢…] n_clusters={n_clusters}, cluster_sizes_keys={list(cluster_sizes.keys())[:10] if cluster_sizes else []}, labels_unique={len(set(labels)) if labels else 0}")
    
    # ì•„í‹°íŒ©íŠ¸ ì €ì¥
    result_data = result.get('data', df)
    
    # labelsë¥¼ numpy arrayë¡œ ë³€í™˜ (save_artifactsê°€ ê¸°ëŒ€í•˜ëŠ” í˜•ì‹)
    labels_array = np.array(labels) if labels else None
    
    # í”¼ì²˜ íƒ€ì… ì •ë³´ ì¶”ì¶œ
    from app.clustering.data_preprocessor import get_feature_types
    feature_types = get_feature_types(result_data)
    
    logger.info(f"[í”¼ì²˜ íƒ€ì… ì¶”ì¶œ] bin: {len(feature_types.get('bin_cols', []))}, cat: {len(feature_types.get('cat_cols', []))}, num: {len(feature_types.get('num_cols', []))}")
    
    save_artifacts(
        session_dir,
        result_data,
        labels_array,
        {
            'request': req.dict(),
            'feature_types': feature_types,  # í”¼ì²˜ íƒ€ì… ì •ë³´ ì¶”ê°€
            'result_meta': {
                'success': result.get('success', False),
                'n_clusters': result.get('n_clusters'),
                'optimal_k': result.get('optimal_k'),
                'k_scores': result.get('k_scores', []),
                'strategy': result.get('strategy'),
                'filter_info': result.get('filter_info'),
                'processor_info': result.get('processor_info'),
                'algorithm_info': {
                    **(result.get('algorithm_info', {}) or {}),
                    'features': result.get('features', []),
                }
            }
        }
    )
    
    logger.info(f"[ì„¸ì…˜ ìƒì„±] {session_id}")
    
    # ë©”íŠ¸ë¦­ ì¶”ì¶œ (ì—¬ëŸ¬ ê²½ë¡œ ì‹œë„)
    silhouette_score = result.get('silhouette_score') or result.get('algorithm_info', {}).get('silhouette_score')
    davies_bouldin_score = result.get('davies_bouldin_score') or result.get('algorithm_info', {}).get('davies_bouldin_score')
    calinski_harabasz = result.get('calinski_harabasz_score') or result.get('algorithm_info', {}).get('calinski_harabasz')
    
    logger.info(f"[ë©”íŠ¸ë¦­ ì¶”ì¶œ] silhouette={silhouette_score}, davies_bouldin={davies_bouldin_score}, calinski={calinski_harabasz}")
    
    return {
        "success": True,
        "session_id": session_id,
        "n_samples": result.get('n_samples', len(df)),
        "n_clusters": n_clusters,  # ê³„ì‚°ëœ n_clusters ì‚¬ìš©
        "labels": labels,
        "cluster_sizes": cluster_sizes,
        "silhouette_score": silhouette_score,
        "davies_bouldin_score": davies_bouldin_score,
        "calinski_harabasz_score": calinski_harabasz,
        "optimal_k": result.get('optimal_k'),
        "strategy": result.get('strategy'),
        "meta": {
            "filter_info": result.get('filter_info'),
            "processor_info": result.get('processor_info'),
            "algorithm_info": {
                **(result.get('algorithm_info', {}) or {}),
                "features": result.get('features', []),
                "silhouette_score": silhouette_score,
                "davies_bouldin_score": davies_bouldin_score,
                "calinski_harabasz": calinski_harabasz,
            }
        },
        "debug": debug_info
    }


@router.post("/cluster")
async def cluster_panels(
    req: ClusterRequest,
    session: AsyncSession = Depends(get_session)
):
    """
    í´ëŸ¬ìŠ¤í„°ë§ ì‹¤í–‰ (DBì—ì„œ ë°ì´í„° ì¶”ì¶œ)
    """
    logger = logging.getLogger(__name__)
    
    debug_info = {
        'step': 'start',
        'panel_ids_count': len(req.panel_ids),
        'errors': []
    }
    
    try:
        logger.info(f"[í´ëŸ¬ìŠ¤í„°ë§ ì‹œì‘] íŒ¨ë„ ìˆ˜: {len(req.panel_ids)}")
        debug_info['step'] = 'extract_data'
        
        # 1. íŒ¨ë„ ë°ì´í„° ì¶”ì¶œ
        panel_data = await extract_features_for_clustering(session, req.panel_ids)
        logger.info(f"[ë°ì´í„° ì¶”ì¶œ] ì¶”ì¶œëœ íŒ¨ë„ ìˆ˜: {len(panel_data) if panel_data else 0}")
        
        if not panel_data:
            debug_info['errors'].append('íŒ¨ë„ ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: DBì—ì„œ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')
            raise HTTPException(
                status_code=404,
                detail=json.dumps({
                    "error": "íŒ¨ë„ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    "debug": debug_info
                }, ensure_ascii=False)
            )
        
        debug_info['step'] = 'preprocess'
        debug_info['raw_data_count'] = len(panel_data)
        
        # 2. ë°ì´í„° ì „ì²˜ë¦¬ (ì›ì‹œ ë°ì´í„° -> í´ëŸ¬ìŠ¤í„°ë§ìš© DataFrame)
        try:
            df = preprocess_for_clustering(panel_data, verbose=False)
            logger.info(f"[ì „ì²˜ë¦¬ ì™„ë£Œ] ì „ì²˜ë¦¬ëœ ë°ì´í„° í–‰ ìˆ˜: {len(df)}, ì—´ ìˆ˜: {len(df.columns) if len(df) > 0 else 0}")
            debug_info['preprocessed_data_count'] = len(df)
            debug_info['preprocessed_columns'] = list(df.columns) if len(df) > 0 else []
        except Exception as preprocess_error:
            debug_info['errors'].append(f'ì „ì²˜ë¦¬ ì‹¤íŒ¨: {str(preprocess_error)}')
            logger.error(f"[ì „ì²˜ë¦¬ ì˜¤ë¥˜] {str(preprocess_error)}", exc_info=True)
            raise HTTPException(
                status_code=400,
                detail=json.dumps({
                    "error": f"ë°ì´í„° ì „ì²˜ë¦¬ ì‹¤íŒ¨: {str(preprocess_error)}",
                    "debug": debug_info
                }, ensure_ascii=False)
            )
        
        if len(df) == 0:
            debug_info['errors'].append('ì „ì²˜ë¦¬ í›„ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.')
            raise HTTPException(
                status_code=400,
                detail=json.dumps({
                    "error": "ì „ì²˜ë¦¬ í›„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.",
                    "debug": debug_info
                }, ensure_ascii=False)
            )
        
        debug_info['step'] = 'check_sample_size'
        debug_info['sample_size'] = len(df)
        
        # ìƒ˜í”Œ ìˆ˜ í™•ì¸ ë° ê²½ê³ 
        if len(df) < 100:
            debug_info['warnings'] = [f'ìƒ˜í”Œ ìˆ˜ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤ ({len(df)}ê°œ < 100ê°œ). ë™ì  ì „ëµì— ë”°ë¼ í”„ë¡œíŒŒì¼ë§ë§Œ ì œê³µë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.']
            logger.warning(f"[ìƒ˜í”Œ ìˆ˜ ë¶€ì¡±] {len(df)}ê°œ íŒ¨ë„ - í”„ë¡œíŒŒì¼ë§ë§Œ ì œê³µë  ìˆ˜ ìˆìŒ")
        
        # ë‚˜ë¨¸ì§€ ë¡œì§ì€ ê³µí†µ í•¨ìˆ˜ë¡œ ë¶„ë¦¬
        return await _execute_clustering(
            df=df,
            req=req,
            debug_info=debug_info,
            logger=logger
        )
        
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        traceback.print_exc()
        debug_info['step'] = 'error'
        debug_info['errors'].append(f'ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}')
        logger.error(f"[í´ëŸ¬ìŠ¤í„°ë§ ì˜¤ë¥˜] {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=json.dumps({
                "error": f"í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨: {str(e)}",
                "debug": debug_info
            }, ensure_ascii=False)
    )


@router.post("/compare")
async def compare_clusters(req: CompareRequest):
    """
    êµ°ì§‘ ë¹„êµ ë¶„ì„
    """
    logger = logging.getLogger(__name__)
    
    try:
        logger.info(f"[ë¹„êµ ë¶„ì„ ì‹œì‘] session_id: {req.session_id}, c1: {req.c1}, c2: {req.c2}")
        
        from app.clustering.compare import compare_groups
        
        # ì„¸ì…˜ì—ì„œ ë°ì´í„° ë¡œë“œ
        from app.clustering.artifacts import load_artifacts
        logger.info(f"[ë¹„êµ ë¶„ì„] ì•„í‹°íŒ©íŠ¸ ë¡œë“œ ì‹œë„: {req.session_id}")
        artifacts = load_artifacts(req.session_id)
        
        if artifacts is None:
            logger.error(f"[ë¹„êµ ë¶„ì„ ì˜¤ë¥˜] ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {req.session_id}")
            raise HTTPException(
                status_code=404,
                detail="ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            )
        
        logger.info(f"[ë¹„êµ ë¶„ì„] ì•„í‹°íŒ©íŠ¸ ë¡œë“œ ì„±ê³µ, keys: {list(artifacts.keys())}")
        
        df = artifacts.get('data')
        labels = artifacts.get('labels')
        
        logger.info(f"[ë¹„êµ ë¶„ì„] ë°ì´í„° í™•ì¸] df: {df is not None}, labels: {labels is not None}")
        if df is not None:
            logger.info(f"[ë¹„êµ ë¶„ì„] DataFrame ì •ë³´] í–‰ ìˆ˜: {len(df)}, ì—´ ìˆ˜: {len(df.columns)}, ì»¬ëŸ¼: {list(df.columns)[:10]}")
        if labels is not None:
            logger.info(f"[ë¹„êµ ë¶„ì„] Labels ì •ë³´] íƒ€ì…: {type(labels)}, ê¸¸ì´: {len(labels) if hasattr(labels, '__len__') else 'N/A'}, ê³ ìœ ê°’: {sorted(set(labels))[:10] if hasattr(labels, '__iter__') else 'N/A'}")
        
        if df is None or labels is None:
            logger.error(f"[ë¹„êµ ë¶„ì„ ì˜¤ë¥˜] ë°ì´í„° ì—†ìŒ] df: {df is None}, labels: {labels is None}")
            raise HTTPException(
                status_code=400,
                detail="í´ëŸ¬ìŠ¤í„°ë§ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
            )
        
        # DataFrameì´ ë¬¸ìì—´ ê²½ë¡œì¸ ê²½ìš° ë¡œë“œ
        if isinstance(df, str):
            logger.info(f"[ë¹„êµ ë¶„ì„] DataFrame ê²½ë¡œì—ì„œ ë¡œë“œ: {df}")
            import pandas as pd
            df = pd.read_csv(df)
            logger.info(f"[ë¹„êµ ë¶„ì„] DataFrame ë¡œë“œ ì™„ë£Œ] í–‰ ìˆ˜: {len(df)}, ì—´ ìˆ˜: {len(df.columns)}")
        
        # labelsë¥¼ numpy arrayë¡œ ë³€í™˜
        if not isinstance(labels, np.ndarray):
            if isinstance(labels, list):
                labels = np.array(labels)
                logger.info(f"[ë¹„êµ ë¶„ì„] Labelsë¥¼ numpy arrayë¡œ ë³€í™˜] ê¸¸ì´: {len(labels)}")
            else:
                logger.error(f"[ë¹„êµ ë¶„ì„ ì˜¤ë¥˜] Labels íƒ€ì… ì˜¤ë¥˜] íƒ€ì…: {type(labels)}")
                raise HTTPException(
                    status_code=400,
                    detail=f"Labels íƒ€ì… ì˜¤ë¥˜: {type(labels)}"
                )
        
        # í´ëŸ¬ìŠ¤í„° ID í™•ì¸
        unique_labels = sorted(set(labels))
        logger.info(f"[ë¹„êµ ë¶„ì„] ê³ ìœ  í´ëŸ¬ìŠ¤í„° ID] {unique_labels[:20]}")
        if req.c1 not in unique_labels:
            logger.warning(f"[ë¹„êµ ë¶„ì„ ê²½ê³ ] í´ëŸ¬ìŠ¤í„° {req.c1}ê°€ labelsì— ì—†ìŒ. ì‚¬ìš© ê°€ëŠ¥í•œ ID: {unique_labels[:10]}")
        if req.c2 not in unique_labels:
            logger.warning(f"[ë¹„êµ ë¶„ì„ ê²½ê³ ] í´ëŸ¬ìŠ¤í„° {req.c2}ê°€ labelsì— ì—†ìŒ. ì‚¬ìš© ê°€ëŠ¥í•œ ID: {unique_labels[:10]}")
        
        # ë¹„êµ ë¶„ì„ ì‹¤í–‰
        # ë©”íƒ€ë°ì´í„°ì—ì„œ í”¼ì²˜ íƒ€ì… ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        feature_types = artifacts.get('meta', {}).get('feature_types', {})
        bin_cols = feature_types.get('bin_cols', [])
        cat_cols = feature_types.get('cat_cols', [])
        num_cols = feature_types.get('num_cols', [])
        
        logger.info(f"[ë¹„êµ ë¶„ì„] í”¼ì²˜ íƒ€ì… ì •ë³´] bin_cols: {len(bin_cols)}, cat_cols: {len(cat_cols)}, num_cols: {len(num_cols)}")
        
        # í”¼ì²˜ íƒ€ì… ì •ë³´ê°€ ì—†ìœ¼ë©´ ìë™ ê°ì§€
        if not bin_cols and not cat_cols and not num_cols:
            logger.info("[ë¹„êµ ë¶„ì„] í”¼ì²˜ íƒ€ì… ìë™ ê°ì§€ ì‹œë„")
            try:
                from app.clustering.data_preprocessor import get_feature_types
                feature_types = get_feature_types(df)
                bin_cols = feature_types.get('bin_cols', [])
                cat_cols = feature_types.get('cat_cols', [])
                num_cols = feature_types.get('num_cols', [])
                logger.info(f"[ë¹„êµ ë¶„ì„] ìë™ ê°ì§€ ì™„ë£Œ] bin_cols: {len(bin_cols)}, cat_cols: {len(cat_cols)}, num_cols: {len(num_cols)}")
            except Exception as e:
                logger.warning(f"[ë¹„êµ ë¶„ì„] í”¼ì²˜ íƒ€ì… ìë™ ê°ì§€ ì‹¤íŒ¨: {str(e)}, ê¸°ë³¸ê°’ ì‚¬ìš©")
                bin_cols = []
                cat_cols = []
                num_cols = []
        
        logger.info(f"[ë¹„êµ ë¶„ì„] compare_groups í˜¸ì¶œ] c1: {req.c1}, c2: {req.c2}")
        comparison = compare_groups(
            df,
            labels,
            req.c1,
            req.c2,
            bin_cols=bin_cols,
            cat_cols=cat_cols,
            num_cols=num_cols
        )
        
        logger.info(f"[ë¹„êµ ë¶„ì„ ì™„ë£Œ] comparison keys: {list(comparison.keys())}, comparison count: {len(comparison.get('comparison', []))}")
        
        return comparison
        
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        error_trace = traceback.format_exc()
        logger.error(f"[ë¹„êµ ë¶„ì„ ì˜¤ë¥˜] {str(e)}\n{error_trace}")
        traceback.print_exc()
        raise HTTPException(
            status_code=500,
            detail=f"ë¹„êµ ë¶„ì„ ì‹¤íŒ¨: {str(e)}"
    )


@router.post("/umap")
async def get_umap_coordinates(req: UMAPRequest):
    """
    UMAP 2D ì¢Œí‘œ ê³„ì‚°
    """
    logger = logging.getLogger(__name__)
    
    try:
        from umap import UMAP
        
        logger.info(f"[UMAP ì‹œì‘] session_id: {req.session_id}, sample: {req.sample}")
        
        # ì„¸ì…˜ì—ì„œ ë°ì´í„° ë¡œë“œ
        from app.clustering.artifacts import load_artifacts
        artifacts = load_artifacts(req.session_id)
        
        if artifacts is None:
            logger.error(f"[UMAP ì˜¤ë¥˜] ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {req.session_id}")
            raise HTTPException(
                status_code=404,
                detail="ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            )
        
        df = artifacts.get('data')
        labels_raw = artifacts.get('labels')
        
        logger.info(f"[UMAP ë°ì´í„° ë¡œë“œ] df: {df is not None}, labels: {labels_raw is not None}")
        if df is not None:
            logger.info(f"[UMAP ë°ì´í„° ì •ë³´] í–‰ ìˆ˜: {len(df)}, ì—´ ìˆ˜: {len(df.columns)}")
        if labels_raw is not None:
            logger.info(f"[UMAP ë¼ë²¨ ì •ë³´] íƒ€ì…: {type(labels_raw)}, ê¸¸ì´: {len(labels_raw) if hasattr(labels_raw, '__len__') else 'N/A'}")
        
        if df is None:
            logger.error("[UMAP ì˜¤ë¥˜] ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            raise HTTPException(
                status_code=400,
                detail="ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
            )

        # labels ì²˜ë¦¬ (numpy arrayì¸ ê²½ìš° ì²˜ë¦¬)
        labels = None
        if labels_raw is not None:
            if isinstance(labels_raw, np.ndarray):
                labels = labels_raw.tolist()
                logger.info(f"[UMAP ë¼ë²¨ ë³€í™˜] numpy array -> list, ê¸¸ì´: {len(labels)}")
            elif isinstance(labels_raw, list):
                labels = labels_raw
                logger.info(f"[UMAP ë¼ë²¨] ì´ë¯¸ list í˜•ì‹, ê¸¸ì´: {len(labels)}")
            else:
                # ë‹¤ë¥¸ íƒ€ì…ì¸ ê²½ìš° ë³€í™˜ ì‹œë„
                try:
                    labels = list(labels_raw)
                    logger.info(f"[UMAP ë¼ë²¨ ë³€í™˜] ë‹¤ë¥¸ íƒ€ì… -> list, ê¸¸ì´: {len(labels)}")
                except Exception as e:
                    logger.warning(f"[UMAP ë¼ë²¨ ë³€í™˜ ì‹¤íŒ¨] {str(e)}")
                    labels = []
        else:
            logger.warning("[UMAP ë¼ë²¨] labelsê°€ Noneì…ë‹ˆë‹¤.")
            labels = []

        # UMAP ì ìš©
        umap = UMAP(
            n_components=2,
            n_neighbors=req.n_neighbors,
            min_dist=req.min_dist,
            metric=req.metric,
            random_state=req.seed
        )
        
        # ìˆ«ìí˜• ì»¬ëŸ¼ë§Œ ì„ íƒ
        numeric_cols = df.select_dtypes(include=['number']).columns.tolist()
        logger.info(f"[UMAP í”¼ì³ ì„ íƒ] ìˆ«ìí˜• ì»¬ëŸ¼: {len(numeric_cols)}ê°œ")
        X = df[numeric_cols].fillna(0).values
        
        # ìƒ˜í”Œë§
        sample_indices = None
        if req.sample and req.sample < len(X):
            np.random.seed(req.seed)
            sample_indices = np.random.choice(len(X), req.sample, replace=False)
            X = X[sample_indices]
            df_sample = df.iloc[sample_indices]
            logger.info(f"[UMAP ìƒ˜í”Œë§] {len(X)}ê°œ ìƒ˜í”Œ ì„ íƒ (ì „ì²´: {len(df)}ê°œ)")
        else:
            df_sample = df
            logger.info(f"[UMAP ìƒ˜í”Œë§] ì „ì²´ ë°ì´í„° ì‚¬ìš©: {len(X)}ê°œ")
        
        # UMAP ë³€í™˜
        logger.info("[UMAP ë³€í™˜ ì‹œì‘]")
        coords = umap.fit_transform(X)
        logger.info(f"[UMAP ë³€í™˜ ì™„ë£Œ] ì¢Œí‘œ ìˆ˜: {len(coords)}")
        
        # labels ìƒ˜í”Œë§ (ìƒ˜í”Œë§ëœ ê²½ìš°)
        if sample_indices is not None and labels:
            sampled_labels = [labels[i] for i in sample_indices]
            logger.info(f"[UMAP ë¼ë²¨ ìƒ˜í”Œë§] {len(sampled_labels)}ê°œ ë¼ë²¨ ì„ íƒ")
        else:
            sampled_labels = labels[:len(coords)] if labels else []
            logger.info(f"[UMAP ë¼ë²¨] ì „ì²´ ë¼ë²¨ ì‚¬ìš©: {len(sampled_labels)}ê°œ")
        
        # panel_ids ì¶”ì¶œ
        if 'mb_sn' in df_sample.columns:
            panel_ids = df_sample['mb_sn'].tolist()
        else:
            panel_ids = df_sample.index.astype(str).tolist()
        
        logger.info(f"[UMAP ì™„ë£Œ] ì¢Œí‘œ: {len(coords)}ê°œ, ë¼ë²¨: {len(sampled_labels)}ê°œ, íŒ¨ë„ID: {len(panel_ids)}ê°œ")
        
        return {
            'coordinates': coords.tolist(),
            'panel_ids': panel_ids,
            'labels': sampled_labels
        }
        
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        error_trace = traceback.format_exc()
        logger.error(f"[UMAP ì˜¤ë¥˜] {str(e)}\n{error_trace}")
        traceback.print_exc()
        raise HTTPException(
            status_code=500,
            detail=f"UMAP ê³„ì‚° ì‹¤íŒ¨: {str(e)}"
        )


class PanelClusterMappingRequest(BaseModel):
    """íŒ¨ë„ IDì™€ í´ëŸ¬ìŠ¤í„° ë§¤ì¹­ ìš”ì²­"""
    session_id: str
    panel_ids: List[str]


@router.post("/panel-cluster-mapping")
async def get_panel_cluster_mapping(req: PanelClusterMappingRequest):
    """
    ê²€ìƒ‰ëœ íŒ¨ë„ IDë“¤ì˜ í´ëŸ¬ìŠ¤í„° ë§¤í•‘ ì •ë³´ ë°˜í™˜
    """
    logger = logging.getLogger(__name__)
    
    try:
        logger.info(f"[íŒ¨ë„-í´ëŸ¬ìŠ¤í„° ë§¤í•‘] session_id: {req.session_id}, panel_ids: {len(req.panel_ids)}ê°œ")
        
        # 1. ë¨¼ì € ì¼ë°˜ ì„¸ì…˜ìœ¼ë¡œ ì‹œë„ (NeonDBì—ì„œ ì§ì ‘ ì¡°íšŒ)
        from app.utils.clustering_loader import load_panel_cluster_mappings_from_db
        from app.clustering.artifacts import load_artifacts
        
        # session_idê°€ ì‹¤ì œ DBì— ìˆëŠ”ì§€ í™•ì¸
        mappings_df = None
        db_session_id = None
        
        # Precomputed ì„¸ì…˜ì¸ ê²½ìš° (precomputed_default, hdbscan_default ë“±)
        is_precomputed = (
            req.session_id == 'precomputed_default' or 
            req.session_id == 'hdbscan_default' or 
            (req.session_id and req.session_id.startswith('precomputed_'))
        )
        
        if is_precomputed:
            logger.info(f"[íŒ¨ë„-í´ëŸ¬ìŠ¤í„° ë§¤í•‘] Precomputed ë°ì´í„° ì‚¬ìš© (NeonDB)")
            from app.utils.clustering_loader import get_precomputed_session_id
            
            # Precomputed ì„¸ì…˜ ID ì¡°íšŒ
            precomputed_name = "hdbscan_default"
            db_session_id = await get_precomputed_session_id(precomputed_name)
            
            if db_session_id:
                logger.info(f"[íŒ¨ë„-í´ëŸ¬ìŠ¤í„° ë§¤í•‘] Precomputed ì„¸ì…˜ ID ì°¾ìŒ: {db_session_id}")
                mappings_df = await load_panel_cluster_mappings_from_db(db_session_id)
        else:
            # ì¼ë°˜ ì„¸ì…˜: UUID í˜•ì‹ ê²€ì¦
            import re
            uuid_pattern = re.compile(r'^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$', re.IGNORECASE)
            
            # UUIDê°€ ì•„ë‹ˆê±°ë‚˜ search_extended_ë¡œ ì‹œì‘í•˜ëŠ” ê²½ìš° precomputedë¡œ fallback
            if not uuid_pattern.match(req.session_id) or req.session_id.startswith('search_extended_'):
                logger.info(f"[íŒ¨ë„-í´ëŸ¬ìŠ¤í„° ë§¤í•‘] UUID í˜•ì‹ì´ ì•„ë‹ˆê±°ë‚˜ search_extended_ ì„¸ì…˜: {req.session_id}, precomputedë¡œ fallback")
                from app.utils.clustering_loader import get_precomputed_session_id
                precomputed_name = "hdbscan_default"
                db_session_id = await get_precomputed_session_id(precomputed_name)
                if db_session_id:
                    logger.info(f"[íŒ¨ë„-í´ëŸ¬ìŠ¤í„° ë§¤í•‘] Precomputed ì„¸ì…˜ ID ì°¾ìŒ: {db_session_id}")
                    mappings_df = await load_panel_cluster_mappings_from_db(db_session_id)
            else:
                # ì¼ë°˜ ì„¸ì…˜: ë¨¼ì € NeonDBì—ì„œ ì§ì ‘ ì¡°íšŒ ì‹œë„
                logger.info(f"[íŒ¨ë„-í´ëŸ¬ìŠ¤í„° ë§¤í•‘] ì¼ë°˜ ì„¸ì…˜ ë°ì´í„° ì‚¬ìš©: {req.session_id}")
                mappings_df = await load_panel_cluster_mappings_from_db(req.session_id)
                if mappings_df is not None and not mappings_df.empty:
                    db_session_id = req.session_id
                    logger.info(f"[íŒ¨ë„-í´ëŸ¬ìŠ¤í„° ë§¤í•‘] NeonDBì—ì„œ ì„¸ì…˜ ë°ì´í„° ì°¾ìŒ: {db_session_id}")
        
        # NeonDBì—ì„œ ë§¤í•‘ì„ ì°¾ì€ ê²½ìš°
        if mappings_df is not None and not mappings_df.empty:
            logger.info(f"[íŒ¨ë„-í´ëŸ¬ìŠ¤í„° ë§¤í•‘] NeonDBì—ì„œ ë§¤í•‘ ë¡œë“œ ì™„ë£Œ: {len(mappings_df)}ê°œ ë§¤í•‘")
            
            # ìš”ì²­ëœ panel_idsì— í•´ë‹¹í•˜ëŠ” ë§¤í•‘ë§Œ í•„í„°ë§
            if req.panel_ids:
                # mb_sn ì •ê·œí™” (ëŒ€ì†Œë¬¸ì, ê³µë°± ì œê±°)
                mappings_df['mb_sn_normalized'] = mappings_df['mb_sn'].astype(str).str.strip().str.lower()
                requested_panel_ids_normalized = [str(pid).strip().lower() for pid in req.panel_ids]
                
                # í•„í„°ë§
                filtered_df = mappings_df[mappings_df['mb_sn_normalized'].isin(requested_panel_ids_normalized)]
                logger.info(f"[íŒ¨ë„-í´ëŸ¬ìŠ¤í„° ë§¤í•‘] ìš”ì²­ëœ {len(req.panel_ids)}ê°œ íŒ¨ë„ ì¤‘ {len(filtered_df)}ê°œ ë§¤í•‘ ì°¾ìŒ")
                
                # panel_to_cluster ìƒì„±
                panel_id_to_cluster = dict(zip(filtered_df['mb_sn_normalized'], filtered_df['cluster']))
                panel_to_cluster = {}
                for panel_id in req.panel_ids:
                    normalized_id = str(panel_id).strip().lower()
                    cluster_id = panel_id_to_cluster.get(normalized_id, -1)
                    panel_to_cluster[str(panel_id).strip()] = int(cluster_id) if cluster_id != -1 else None
            else:
                # ì „ì²´ ë§¤í•‘ ìƒì„±
                panel_to_cluster = {}
                for _, row in mappings_df.iterrows():
                    normalized_id = str(row['mb_sn']).strip()
                    cluster_id = int(row['cluster'])
                    panel_to_cluster[normalized_id] = cluster_id if cluster_id != -1 else None
            
            # ê²°ê³¼ ìƒì„±
            mapping_results = []
            not_found_ids = []
            for panel_id in req.panel_ids:
                normalized_request_id = str(panel_id).strip()
                cluster_id = panel_to_cluster.get(normalized_request_id, None)
                
                # ëŒ€ì†Œë¬¸ì ë¬´ì‹œ ë§¤ì¹­ ì‹œë„
                if cluster_id is None:
                    normalized_lower = normalized_request_id.lower()
                    for key, value in panel_to_cluster.items():
                        if key.lower() == normalized_lower:
                            cluster_id = value
                            break
                
                if cluster_id is not None and cluster_id != -1:
                    mapping_results.append({
                        'panel_id': str(panel_id),
                        'cluster_id': int(cluster_id),
                        'found': True
                    })
                else:
                    not_found_ids.append(str(panel_id))
                    mapping_results.append({
                        'panel_id': str(panel_id),
                        'cluster_id': None,
                        'found': False
                    })
            
            logger.info(f"[íŒ¨ë„-í´ëŸ¬ìŠ¤í„° ë§¤í•‘] ê²°ê³¼: {len(mapping_results)}ê°œ ë§¤í•‘, {len(not_found_ids)}ê°œ ë¯¸ì°¾ìŒ")
            
            return {
                'session_id': req.session_id,
                'mappings': mapping_results,
                'total_requested': len(req.panel_ids),
                'total_found': len(mapping_results) - len(not_found_ids)
            }
        
        # NeonDBì—ì„œ ì°¾ì§€ ëª»í•œ ê²½ìš°: íŒŒì¼ ì‹œìŠ¤í…œ fallback (ì¼ë°˜ ì„¸ì…˜ë§Œ)
        if not is_precomputed:
            # ì¼ë°˜ ì„¸ì…˜: artifactsì—ì„œ ë¡œë“œ
            artifacts = load_artifacts(req.session_id)
            
            if artifacts is None:
                error_msg = f"ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {req.session_id}. NeonDBì™€ íŒŒì¼ ì‹œìŠ¤í…œ ëª¨ë‘ í™•ì¸í–ˆì§€ë§Œ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                logger.error(f"[íŒ¨ë„-í´ëŸ¬ìŠ¤í„° ë§¤í•‘] {error_msg}")
                raise HTTPException(
                    status_code=404,
                    detail=error_msg
                )
            
            df = artifacts.get('data')
            labels_raw = artifacts.get('labels')
            
            if df is None:
                raise HTTPException(
                    status_code=400,
                    detail="ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
                )
            
            # DataFrameì´ ê²½ë¡œ ë¬¸ìì—´ì´ë©´ ë¡œë“œ
            if isinstance(df, str):
                df = pd.read_csv(df)
            
            # labels ì²˜ë¦¬
            labels = None
            if labels_raw is not None:
                if isinstance(labels_raw, np.ndarray):
                    labels = labels_raw.tolist()
                elif isinstance(labels_raw, list):
                    labels = labels_raw
                else:
                    labels = list(labels_raw)
            
            if labels is None or len(labels) == 0:
                # DataFrameì— cluster ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸
                if 'cluster' in df.columns:
                    labels = df['cluster'].tolist()
                else:
                    raise HTTPException(
                        status_code=400,
                        detail="í´ëŸ¬ìŠ¤í„° ë ˆì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                    )
            
            # panel_ids ì¶”ì¶œ (mb_sn ì»¬ëŸ¼ ë˜ëŠ” ì¸ë±ìŠ¤)
            if 'mb_sn' in df.columns:
                df_panel_ids = df['mb_sn'].tolist()
            else:
                df_panel_ids = df.index.astype(str).tolist()
            
            logger.info(f"[íŒ¨ë„-í´ëŸ¬ìŠ¤í„° ë§¤í•‘] ë°ì´í„°í”„ë ˆì„ íŒ¨ë„ ìˆ˜: {len(df_panel_ids)}, ë ˆì´ë¸” ìˆ˜: {len(labels)}")
            
            # íŒ¨ë„ IDì™€ í´ëŸ¬ìŠ¤í„° ë§¤í•‘ ìƒì„± (ì •ê·œí™”ëœ í‚¤ë¡œ ì €ì¥)
            panel_to_cluster = {}
            for idx, panel_id in enumerate(df_panel_ids):
                if idx < len(labels):
                    # ì •ê·œí™”: ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ê³  ê³µë°± ì œê±°
                    normalized_id = str(panel_id).strip()
                    cluster_id = int(labels[idx])
                    panel_to_cluster[normalized_id] = cluster_id if cluster_id != -1 else None
            
            logger.info(f"[íŒ¨ë„-í´ëŸ¬ìŠ¤í„° ë§¤í•‘] ë§¤í•‘ í…Œì´ë¸” ìƒì„± ì™„ë£Œ: {len(panel_to_cluster)}ê°œ íŒ¨ë„")
            
            # ê²°ê³¼ ìƒì„±
            mapping_results = []
            not_found_ids = []
            for panel_id in req.panel_ids:
                normalized_request_id = str(panel_id).strip()
                cluster_id = panel_to_cluster.get(normalized_request_id, None)
                
                # ëŒ€ì†Œë¬¸ì ë¬´ì‹œ ë§¤ì¹­ ì‹œë„
                if cluster_id is None:
                    normalized_lower = normalized_request_id.lower()
                    for key, value in panel_to_cluster.items():
                        if key.lower() == normalized_lower:
                            cluster_id = value
                            logger.debug(f"[íŒ¨ë„-í´ëŸ¬ìŠ¤í„° ë§¤í•‘] ëŒ€ì†Œë¬¸ì ë¬´ì‹œ ë§¤ì¹­ ì„±ê³µ: '{normalized_request_id}' -> '{key}'")
                            break
                
                if cluster_id is not None:
                    mapping_results.append({
                        'panel_id': panel_id,
                        'cluster_id': int(cluster_id),
                        'found': True
                    })
                else:
                    not_found_ids.append(normalized_request_id)
                    logger.warning(f"[íŒ¨ë„-í´ëŸ¬ìŠ¤í„° ë§¤í•‘] íŒ¨ë„ ID ë§¤ì¹­ ì‹¤íŒ¨: ì›ë³¸='{panel_id}', ì •ê·œí™”='{normalized_request_id}'")
                    mapping_results.append({
                        'panel_id': panel_id,
                        'cluster_id': None,
                        'found': False
                    })
            
            found_count = sum(1 for r in mapping_results if r['found'])
            logger.info(f"[íŒ¨ë„-í´ëŸ¬ìŠ¤í„° ë§¤í•‘] ë§¤ì¹­ ì™„ë£Œ: {found_count}/{len(req.panel_ids)}ê°œ íŒ¨ë„ ì°¾ìŒ")
            
            return {
                'session_id': req.session_id,
                'mappings': mapping_results,
                'total_requested': len(req.panel_ids),
                'total_found': found_count
            }
        else:
            # Precomputed ì„¸ì…˜ì¸ë° NeonDBì—ì„œ ì°¾ì§€ ëª»í•œ ê²½ìš°
            error_msg = f"Precomputed ì„¸ì…˜ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {req.session_id}. NeonDBì— ë°ì´í„°ê°€ ë§ˆì´ê·¸ë ˆì´ì…˜ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”."
            logger.error(f"[íŒ¨ë„-í´ëŸ¬ìŠ¤í„° ë§¤í•‘] {error_msg}")
            raise HTTPException(
                status_code=404,
                detail=error_msg
            )
        
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        error_trace = traceback.format_exc()
        logger.error(f"[íŒ¨ë„-í´ëŸ¬ìŠ¤í„° ë§¤í•‘ ì˜¤ë¥˜] {str(e)}\n{error_trace}")
        traceback.print_exc()
        raise HTTPException(
            status_code=500,
            detail=f"íŒ¨ë„-í´ëŸ¬ìŠ¤í„° ë§¤í•‘ ì‹¤íŒ¨: {str(e)}"
        )


# ë””ë²„ê·¸: íŒŒì¼ ë¡œë“œ ì™„ë£Œ í™•ì¸


# ============================================
# ê²€ìƒ‰ ê²°ê³¼ ì£¼ë³€ í´ëŸ¬ìŠ¤í„°ë§ ê´€ë ¨ í•¨ìˆ˜
# ============================================


class ClusterAroundSearchRequest(BaseModel):
    """ê²€ìƒ‰ ê²°ê³¼ ì£¼ë³€ í´ëŸ¬ìŠ¤í„°ë§ ìš”ì²­"""
    search_panel_ids: List[str]
    k_neighbors_per_panel: int = 100  # ê° ê²€ìƒ‰ íŒ¨ë„ë‹¹ ì´ì›ƒ ìˆ˜


@router.post("/cluster-around-search")
async def cluster_around_search(req: ClusterAroundSearchRequest):
    """
    ê²€ìƒ‰ ê²°ê³¼ ì£¼ë³€ ìœ ì‚¬ íŒ¨ë„ ì¡°íšŒ (HDBSCAN ê²°ê³¼ ì¬ì‚¬ìš©)
    - Precomputed HDBSCAN ê²°ê³¼ì—ì„œ ê²€ìƒ‰ëœ íŒ¨ë„ì´ ì†í•œ í´ëŸ¬ìŠ¤í„° ì°¾ê¸°
    - í•´ë‹¹ í´ëŸ¬ìŠ¤í„°ì˜ ëª¨ë“  íŒ¨ë„ ë°˜í™˜ (ì¬í´ëŸ¬ìŠ¤í„°ë§ ì—†ìŒ)
    - Precomputed UMAP ì¢Œí‘œ ì¬ì‚¬ìš©
    """
    logger = logging.getLogger(__name__)
    
    try:
        logger.info("=" * 80)
        logger.info(f"[ğŸ” í™•ì¥ í´ëŸ¬ìŠ¤í„°ë§ API í˜¸ì¶œ]")
        logger.info(f"[ğŸ“‹ ìš”ì²­ ë°ì´í„°] ê²€ìƒ‰ íŒ¨ë„: {len(req.search_panel_ids)}ê°œ, ì´ì›ƒ ìˆ˜: {req.k_neighbors_per_panel}ê°œ")
        logger.info(f"[ğŸ“‹ ê²€ìƒ‰ íŒ¨ë„ ID ìƒ˜í”Œ] {req.search_panel_ids[:10]}")
        logger.info(f"[ğŸ“‹ ê²€ìƒ‰ íŒ¨ë„ ID íƒ€ì…] {[type(pid).__name__ for pid in req.search_panel_ids[:5]]}")
        logger.info("=" * 80)
        
        # 1. Precomputed HDBSCAN ë°ì´í„° ë¡œë“œ (NeonDBì—ì„œ ì¡°íšŒ)
        # âœ… ìµœì í™”: ì›ë³¸ ë°ì´í„° ë¡œë“œ ë¶ˆí•„ìš”, Precomputed UMAP ì¢Œí‘œì™€ í´ëŸ¬ìŠ¤í„° ë§¤í•‘ë§Œ ì‚¬ìš©
        logger.info(f"[1ë‹¨ê³„] Precomputed ë°ì´í„° ë¡œë“œ ì‹œì‘ (NeonDB)")
        from app.utils.clustering_loader import get_precomputed_session_id, load_umap_coordinates_from_db, load_panel_cluster_mappings_from_db
        
        # Precomputed ì„¸ì…˜ ID ì¡°íšŒ
        precomputed_name = "hdbscan_default"
        session_id = await get_precomputed_session_id(precomputed_name)
        
        if not session_id:
            error_msg = f"Precomputed ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: name={precomputed_name}. NeonDBì— ë°ì´í„°ê°€ ë§ˆì´ê·¸ë ˆì´ì…˜ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”."
            logger.error(f"[í™•ì¥ í´ëŸ¬ìŠ¤í„°ë§] {error_msg}")
            raise HTTPException(status_code=404, detail=error_msg)
        
        logger.info(f"[í™•ì¥ í´ëŸ¬ìŠ¤í„°ë§] Precomputed ì„¸ì…˜ ID ì°¾ìŒ: {session_id}")
        
        # UMAP ì¢Œí‘œ ë¡œë“œ
        umap_df = await load_umap_coordinates_from_db(session_id)
        if umap_df is None or umap_df.empty:
            error_msg = f"NeonDBì—ì„œ UMAP ì¢Œí‘œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: session_id={session_id}"
            logger.error(f"[í™•ì¥ í´ëŸ¬ìŠ¤í„°ë§] {error_msg}")
            raise HTTPException(status_code=404, detail=error_msg)
        
        # í´ëŸ¬ìŠ¤í„° ë§¤í•‘ ë¡œë“œ
        cluster_df = await load_panel_cluster_mappings_from_db(session_id)
        if cluster_df is None or cluster_df.empty:
            logger.warning(f"[í™•ì¥ í´ëŸ¬ìŠ¤í„°ë§] í´ëŸ¬ìŠ¤í„° ë§¤í•‘ ë°ì´í„° ì—†ìŒ, -1ë¡œ ì„¤ì •")
            cluster_df = pd.DataFrame(columns=['mb_sn', 'cluster'])
        
        # ë°ì´í„° ë³‘í•©
        if not cluster_df.empty:
            df_precomputed = umap_df.merge(cluster_df, on='mb_sn', how='left')
            df_precomputed['cluster'] = df_precomputed['cluster'].fillna(-1).astype(int)
        else:
            df_precomputed = umap_df.copy()
            df_precomputed['cluster'] = -1
        
        logger.info(f"[í™•ì¥ í´ëŸ¬ìŠ¤í„°ë§] Precomputed ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df_precomputed)}í–‰")
        
        # 2. ê²€ìƒ‰ íŒ¨ë„ ì°¾ê¸° (df_precomputedì—ì„œ ì§ì ‘ ì°¾ê¸°)
        logger.info(f"[2ë‹¨ê³„] ê²€ìƒ‰ íŒ¨ë„ ì°¾ê¸° ì‹œì‘ (Precomputed ë°ì´í„°ì—ì„œ ì§ì ‘ ì¡°íšŒ)")
        logger.info(f"  - ìš”ì²­ëœ íŒ¨ë„ ID ìˆ˜: {len(req.search_panel_ids)}ê°œ")
        logger.info(f"  - ìš”ì²­ëœ íŒ¨ë„ ID ìƒ˜í”Œ: {req.search_panel_ids[:5]}")
        
        # df_precomputedì˜ mb_snì„ ì •ê·œí™”í•˜ì—¬ ë§¤ì¹­ í…Œì´ë¸” ìƒì„±
        df_precomputed['mb_sn_normalized'] = df_precomputed['mb_sn'].astype(str).str.strip().str.lower()
        precomputed_panel_set = set(df_precomputed['mb_sn_normalized'].unique())
        
        logger.info(f"[2ë‹¨ê³„] Precomputed ë°ì´í„° íŒ¨ë„ ìˆ˜: {len(precomputed_panel_set)}ê°œ")
        logger.info(f"[2ë‹¨ê³„] Precomputed íŒ¨ë„ ID ìƒ˜í”Œ: {list(precomputed_panel_set)[:10]}")
        
        # ê²€ìƒ‰ëœ íŒ¨ë„ì˜ mb_sn ì¶”ì¶œ (ì •ê·œí™”)
        search_panel_mb_sns = set()
        not_found_panels = []
        found_panels = []
        
        for panel_id in req.search_panel_ids:
            panel_id_normalized = str(panel_id).strip().lower()
            
            if panel_id_normalized in precomputed_panel_set:
                search_panel_mb_sns.add(panel_id_normalized)
                found_panels.append(panel_id)
            else:
                # ë¶€ë¶„ ë§¤ì¹­ ì‹œë„ (ì• 10ìë¦¬ë§Œ ë¹„êµ)
                panel_id_prefix = panel_id_normalized[:10] if len(panel_id_normalized) > 10 else panel_id_normalized
                matching_panels = [p for p in precomputed_panel_set if panel_id_prefix in p or p in panel_id_prefix]
                
                if matching_panels:
                    search_panel_mb_sns.add(matching_panels[0])
                    found_panels.append(panel_id)
                else:
                    not_found_panels.append(panel_id)
        
        logger.info(f"[2ë‹¨ê³„ ê²°ê³¼]")
        logger.info(f"  - ì°¾ì€ íŒ¨ë„: {len(found_panels)}ê°œ")
        logger.info(f"  - ì°¾ì§€ ëª»í•œ íŒ¨ë„: {len(not_found_panels)}ê°œ")
        logger.info(f"  - ì°¾ì€ íŒ¨ë„ ìƒ˜í”Œ: {found_panels[:5]}")
        if not_found_panels:
            logger.warning(f"  - ì°¾ì§€ ëª»í•œ íŒ¨ë„ ìƒ˜í”Œ: {not_found_panels[:5]}")
        
        # ë§¤ì¹­ ì‹¤íŒ¨ ì‹œ ì „ì²´ precomputed ë°ì´í„° ë°˜í™˜
        if len(search_panel_mb_sns) == 0:
            logger.warning(f"[âš ï¸ 2ë‹¨ê³„] ëª¨ë“  íŒ¨ë„ì„ ì°¾ì§€ ëª»í•¨ - ì „ì²´ precomputed ë°ì´í„° ë°˜í™˜")
            requested_set = set(str(pid).strip().lower() for pid in req.search_panel_ids)
            common = requested_set & precomputed_panel_set
            
            logger.warning(f"  - ìš”ì²­ëœ ID ìˆ˜: {len(requested_set)}")
            logger.warning(f"  - Precomputed ë°ì´í„° ID ìˆ˜: {len(precomputed_panel_set)}")
            logger.warning(f"  - ê²¹ì¹˜ëŠ” ID ìˆ˜: {len(common)}")
            logger.warning(f"  - ê²¹ì¹˜ì§€ ì•ŠëŠ” ìš”ì²­ ID ìƒ˜í”Œ: {list(requested_set - precomputed_panel_set)[:10]}")
            
            # ì „ì²´ precomputed ë°ì´í„° ë°˜í™˜
            logger.info(f"[ì „ì²´ ë°ì´í„° ë°˜í™˜] precomputed UMAP ë°ì´í„° ì „ì²´ ë°˜í™˜")
            
            has_cluster_col = 'cluster' in df_precomputed.columns
            
            result_panels = []
            for _, row in df_precomputed.iterrows():
                panel_id = str(row['mb_sn']).strip()
                is_search = panel_id.lower() in requested_set
                cluster_value = int(row['cluster']) if has_cluster_col else -1
                
                result_panels.append({
                    'panel_id': panel_id,
                    'umap_x': float(row['umap_x']),
                    'umap_y': float(row['umap_y']),
                    'cluster': cluster_value,
                    'is_search_result': bool(is_search),
                    'original_cluster': cluster_value
                })
            
            cluster_stats = {}
            if has_cluster_col:
                for cluster_id in df_precomputed['cluster'].unique():
                    cluster_mask = df_precomputed['cluster'] == cluster_id
                    cluster_panels = [p for p in result_panels if p['cluster'] == cluster_id]
                    search_count = sum(1 for p in cluster_panels if p['is_search_result'])
                    
                    cluster_stats[int(cluster_id)] = {
                        'size': int(cluster_mask.sum()),
                        'percentage': float(cluster_mask.sum() / len(df_precomputed) * 100),
                        'search_count': search_count,
                        'search_percentage': float(search_count / max(1, cluster_mask.sum()) * 100)
                    }
            
            return {
                'success': True,
                'session_id': 'precomputed_default',
                'n_total_panels': len(result_panels),
                'n_search_panels': 0,
                'n_extended_panels': 0,
                'n_clusters': len(cluster_stats),
                'silhouette_score': None,
                'panels': result_panels,
                'cluster_stats': cluster_stats,
                'features_used': [],
                'dispersion_warning': False,
                'dispersion_ratio': 1.0,
                'warning': f'ê²€ìƒ‰ íŒ¨ë„ì„ í´ëŸ¬ìŠ¤í„°ë§ ë°ì´í„°ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ì–´ ì „ì²´ ë°ì´í„°ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤. ìš”ì²­ëœ {len(requested_set)}ê°œ ì¤‘ {len(common)}ê°œë§Œ ë°ì´í„°ì— ì¡´ì¬í•©ë‹ˆë‹¤.'
            }
        
        if len(not_found_panels) > 0:
            logger.warning(f"[âš ï¸ 2ë‹¨ê³„ ê²½ê³ ] {len(not_found_panels)}ê°œ íŒ¨ë„ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (ê³„ì† ì§„í–‰)")
        
        logger.info(f"[âœ… 2ë‹¨ê³„ ì™„ë£Œ] ê²€ìƒ‰ íŒ¨ë„ ì°¾ê¸° ì™„ë£Œ: {len(search_panel_mb_sns)}ê°œ")
        
        # 3. Precomputed HDBSCAN ê²°ê³¼ì—ì„œ ê²€ìƒ‰ëœ íŒ¨ë„ì´ ì†í•œ í´ëŸ¬ìŠ¤í„° ì°¾ê¸° (ì¬í´ëŸ¬ìŠ¤í„°ë§ ì—†ì´)
        logger.info(f"[3ë‹¨ê³„] HDBSCAN ê²°ê³¼ì—ì„œ ê²€ìƒ‰ëœ íŒ¨ë„ì˜ í´ëŸ¬ìŠ¤í„° ì°¾ê¸° (ì¬í´ëŸ¬ìŠ¤í„°ë§ ì—†ìŒ)")
        
        # Precomputed ë°ì´í„°ì—ì„œ ê²€ìƒ‰ëœ íŒ¨ë„ì´ ì†í•œ í´ëŸ¬ìŠ¤í„° ì°¾ê¸°
        has_cluster_col = 'cluster' in df_precomputed.columns
        searched_cluster_ids = set()
        
        if has_cluster_col:
            for _, row in df_precomputed.iterrows():
                panel_id = str(row['mb_sn_normalized']).lower()
                if panel_id in search_panel_mb_sns:
                    cluster_id = int(row['cluster'])
                    if cluster_id != -1:  # ë…¸ì´ì¦ˆ ì œì™¸
                        searched_cluster_ids.add(cluster_id)
        else:
            logger.warning(f"[3ë‹¨ê³„] cluster ì»¬ëŸ¼ì´ ì—†ì–´ í´ëŸ¬ìŠ¤í„° ê¸°ë°˜ í™•ì¥ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        logger.info(f"[3ë‹¨ê³„ ì™„ë£Œ] ê²€ìƒ‰ëœ íŒ¨ë„ì´ ì†í•œ í´ëŸ¬ìŠ¤í„°: {sorted(searched_cluster_ids)}")
        
        # 4. í•´ë‹¹ í´ëŸ¬ìŠ¤í„°ì˜ ëª¨ë“  íŒ¨ë„ ì¶”ì¶œ (ì¬í´ëŸ¬ìŠ¤í„°ë§ ì—†ì´ HDBSCAN ê²°ê³¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©)
        extended_panel_ids = set()
        if has_cluster_col:
            for _, row in df_precomputed.iterrows():
                panel_id = str(row['mb_sn_normalized']).lower()
                cluster_id = int(row['cluster'])
                if cluster_id in searched_cluster_ids:
                    extended_panel_ids.add(panel_id)
        else:
            # cluster ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ê²€ìƒ‰ëœ íŒ¨ë„ë§Œ í¬í•¨
            extended_panel_ids = search_panel_mb_sns.copy()
        
        logger.info(f"[HDBSCAN ê²°ê³¼ ì‚¬ìš©] ì¬í´ëŸ¬ìŠ¤í„°ë§ ì—†ì´ ê¸°ì¡´ HDBSCAN ê²°ê³¼ ì‚¬ìš©")
        logger.info(f"  - ê²€ìƒ‰ íŒ¨ë„: {len(search_panel_mb_sns)}ê°œ")
        logger.info(f"  - í´ëŸ¬ìŠ¤í„° ìˆ˜: {len(searched_cluster_ids)}ê°œ")
        logger.info(f"  - í™•ì¥ íŒ¨ë„: {len(extended_panel_ids)}ê°œ")
        
        # 5. ê²°ê³¼ êµ¬ì„± (ì •ìƒì ìœ¼ë¡œ ë§¤ì¹­ëœ ê²€ìƒ‰ íŒ¨ë„ë§Œ í¬í•¨)
        result_panels = []
        
        # ê²€ìƒ‰ëœ íŒ¨ë„ ì¤‘ ì •ìƒì ìœ¼ë¡œ ë§¤ì¹­ëœ íŒ¨ë„ë§Œ UMAPì— í‘œì‹œ
        for panel_id in found_panels:
            panel_id_normalized = str(panel_id).strip().lower()
            
            # df_precomputedì—ì„œ í•´ë‹¹ íŒ¨ë„ ì°¾ê¸°
            matching_rows = df_precomputed[df_precomputed['mb_sn_normalized'] == panel_id_normalized]
            
            if not matching_rows.empty:
                row = matching_rows.iloc[0]
                cluster_id = int(row['cluster']) if has_cluster_col else -1
                
                result_panels.append({
                    'panel_id': str(panel_id).strip(),
                    'umap_x': float(row['umap_x']),
                    'umap_y': float(row['umap_y']),
                    'cluster': cluster_id,
                    'is_search_result': True,  # ê²€ìƒ‰ëœ íŒ¨ë„ì´ë¯€ë¡œ í•­ìƒ True
                    'original_cluster': cluster_id
                })
        
        logger.info(f"[5ë‹¨ê³„] ì •ìƒì ìœ¼ë¡œ ë§¤ì¹­ëœ ê²€ìƒ‰ íŒ¨ë„: {len(result_panels)}ê°œ")
        
        # 7. í´ëŸ¬ìŠ¤í„°ë³„ í†µê³„ (ì •ìƒì ìœ¼ë¡œ ë§¤ì¹­ëœ ê²€ìƒ‰ íŒ¨ë„ ê¸°ì¤€)
        cluster_stats = {}
        if result_panels:
            # result_panelsì˜ í´ëŸ¬ìŠ¤í„°ë³„ë¡œ í†µê³„ ê³„ì‚°
            cluster_panel_map = {}
            for panel in result_panels:
                cluster_id = panel['cluster']
                if cluster_id not in cluster_panel_map:
                    cluster_panel_map[cluster_id] = []
                cluster_panel_map[cluster_id].append(panel)
            
            for cluster_id, cluster_panels in cluster_panel_map.items():
                search_count = sum(1 for p in cluster_panels if p['is_search_result'])
                
                cluster_stats[int(cluster_id)] = {
                    'size': len(cluster_panels),
                    'percentage': float(len(cluster_panels) / len(result_panels) * 100) if result_panels else 0.0,
                    'search_count': search_count,
                    'search_percentage': float(search_count / max(1, len(cluster_panels)) * 100)
                }
        
        best_k = len(searched_cluster_ids)
        
        # 8. HDBSCAN ë©”íƒ€ë°ì´í„°ì—ì„œ í’ˆì§ˆ ì§€í‘œ ê°€ì ¸ì˜¤ê¸° (NeonDBì—ì„œ)
        from app.utils.clustering_loader import load_clustering_session_from_db
        quality_metrics = {}
        try:
            session_data = await load_clustering_session_from_db(session_id)
            if session_data:
                quality_metrics['silhouette_score'] = session_data.get('silhouette_score')
                quality_metrics['davies_bouldin_score'] = session_data.get('davies_bouldin_score')
                quality_metrics['calinski_harabasz_score'] = session_data.get('calinski_harabasz_score')
        except Exception as e:
            logger.warning(f"[í’ˆì§ˆ ì§€í‘œ ë¡œë“œ] ì‹¤íŒ¨: {str(e)}")
        
        # 9. ì„¸ì…˜ ID ìƒì„± (ì°¸ê³ ìš©, ì‹¤ì œë¡œëŠ” precomputed ë°ì´í„° ì‚¬ìš©)
        session_id = f"search_extended_{uuid.uuid4().hex[:8]}"
        logger.info(f"[HDBSCAN ê²°ê³¼ ì‚¬ìš© ì™„ë£Œ] ì„¸ì…˜ ID: {session_id}, í´ëŸ¬ìŠ¤í„° ìˆ˜: {best_k} (ì¬í´ëŸ¬ìŠ¤í„°ë§ ì—†ìŒ)")
        
        return {
            'success': True,
            'session_id': session_id,
            'n_total_panels': len(result_panels),
            'n_search_panels': len(search_panel_mb_sns),
            'n_extended_panels': len(extended_panel_ids) - len(search_panel_mb_sns),
            'n_clusters': best_k,
            'silhouette_score': quality_metrics.get('silhouette_score'),
            'davies_bouldin_score': quality_metrics.get('davies_bouldin_score'),
            'calinski_harabasz_score': quality_metrics.get('calinski_harabasz_score'),
            'panels': result_panels,
            'cluster_stats': cluster_stats,
            'features_used': [],  # Precomputed ë°ì´í„° ì‚¬ìš© ì‹œ í”¼ì²˜ ì •ë³´ ë¶ˆí•„ìš”
            'dispersion_warning': False,
            'dispersion_ratio': 1.0,
            'method': 'HDBSCAN (precomputed, no re-clustering)',
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"[í™•ì¥ í´ëŸ¬ìŠ¤í„°ë§ ì˜¤ë¥˜] {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"í™•ì¥ í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨: {str(e)}"
        )
